{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Quantum Hamiltonians with Shallow Circuits using Decision Diagrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5 (default, Jan 27 2021, 15:41:15) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import decimal as d\n",
    "from typing import Dict, List, Callable, Hashable, Tuple, Set, Optional\n",
    "try:\n",
    "    from math import prod\n",
    "except ImportError:\n",
    "    from functools import reduce \n",
    "    import operator\n",
    "    def prod(iterable):\n",
    "        return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "PauliTerm = str\n",
    "PauliOp = str\n",
    "Hamiltonian = Dict[PauliTerm, d.Decimal]\n",
    "DDSampler = Callable[[nx.MultiDiGraph, Hashable], Tuple[d.Decimal, PauliTerm]]\n",
    "Node = Hashable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_included_terms(ppaths: List[PauliTerm], meas: PauliTerm) -> List[PauliTerm]:\n",
    "    \"\"\"return set of keys included by measurement basis\"\"\"\n",
    "    answer = []\n",
    "    for p in ppaths:\n",
    "        if len(p) != len(meas):\n",
    "            continue\n",
    "        else:\n",
    "            is_included = True\n",
    "            for i in range(len(p)):\n",
    "                if p[i] != \"I\" and p[i] != meas[i]:\n",
    "                    is_included = False\n",
    "                    break\n",
    "            if is_included:\n",
    "                answer.append(p)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Shot Variance\n",
    "\n",
    "$P = \\{I, X, Y, Z \\}$\n",
    "\n",
    "$\\mathcal{P} = \\hat{P} = \\{X, Y, Z \\}$ \n",
    "\n",
    "- $\\mathcal{P}$ is hard to distinctively write by hand, so if my notes have $\\hat{P}$ somewhere it's the same as $\\mathcal{P}$\n",
    "\n",
    "$\\operatorname{Var}(\\nu)\n",
    "    =\n",
    "    \\underbrace{\\left(\n",
    "    \\sum_{P,Q} \\alpha_P \\alpha_Q \n",
    "        g(P,Q,\\beta)\n",
    "            \\operatorname{Tr}(PQ\\rho)\n",
    "    \\right)}_{\\text{first part}}\n",
    "    -\n",
    "    {\\underbrace{\\left(\n",
    "    \\operatorname{Tr}(H\\rho)\n",
    "    \\right)}_\\text{second part}}^2$\n",
    "    \n",
    "- Better names for *first_part* and *second_part*?\n",
    "    \n",
    "$\\operatorname{Tr}(PQ\\rho) = \\langle\\psi_\\rho| \\operatorname{multiply\\_terms}(P,Q) |\\psi_\\rho\\rangle$ \n",
    "\n",
    "$g(P, Q, \\beta) \n",
    "   = \\frac{1}{\\zeta(P, \\beta)} \\frac{1}{\\zeta(Q, \\beta)} \\sum_{B \\in \\operatorname{Lift}(P) \\cap \\operatorname{Lift}(Q)}\\beta(B) \n",
    "   = \\frac{1}{\\zeta(P, \\beta)} \\frac{1}{\\zeta(Q, \\beta)} \\zeta(\\operatorname{merge\\_terms}(P,Q), \\beta)$\n",
    "   \n",
    "- What should happen if one of the $\\zeta$s in the divisor is 0?\n",
    "\n",
    "$\\zeta(P, \\beta) = \\sum_{B \\in \\operatorname{Lift}(P)} \\beta(B)$\n",
    "\n",
    "- The $\\zeta$ function is the same as `dd.compute_covered_prob(graph, 0, P)` for coefficient-DD (0 means starting at the top node)\n",
    "- The $\\zeta$ function is the same as `dd.count_covering_paths(graph, 0, P)) / total_paths` for uniform-path-DD (0 means starting at the top node)\n",
    "\n",
    "$\\operatorname{Lift}(P) = \\{ B \\in \\mathcal{P}^n \\mid B_i = P_i \\text{ for every } i \\in \\operatorname{supp}(P) \\}$\n",
    "\n",
    "$\\operatorname{supp}(Q) = \\{ i \\mid Q_i \\neq I \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import pathlib\n",
    "import copy\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import qutip\n",
    "\n",
    "import src.sampling as dd\n",
    "\n",
    "\n",
    "def read_hamiltonian(filename: str) -> Hamiltonian:\n",
    "    H = {}\n",
    "    lineno = 0\n",
    "    with open(filename, \"r\") as fp:\n",
    "        for line in fp:\n",
    "            if lineno % 2 == 0:\n",
    "                key = line.rstrip()\n",
    "            else:\n",
    "                val = d.Decimal(line.rstrip())\n",
    "                H[key] = val\n",
    "            lineno += 1\n",
    "    return H\n",
    "\n",
    "\n",
    "def convert_H_to_matrix(H: Hamiltonian):\n",
    "    \"\"\"converting dictionary to matrix, and returning the j-th eigenpairs\"\"\"\n",
    "    paulis = {\"I\": qutip.identity(2),\n",
    "              \"X\": qutip.sigmax(),\n",
    "              \"Y\": qutip.sigmay(),\n",
    "              \"Z\": qutip.sigmaz()}\n",
    "    for k in H.keys():\n",
    "        nQubits = len(k)\n",
    "        break\n",
    "        \n",
    "    matrix = None\n",
    "    for pauli_term, coefficient in H.items():\n",
    "        if matrix is None:\n",
    "            matrix = float(coefficient) * qutip.tensor( [ paulis[p] for p in pauli_term ] )\n",
    "        else:\n",
    "            matrix += float(coefficient) * qutip.tensor( [ paulis[p] for p in pauli_term ] )\n",
    "            \n",
    "    eigenpair = matrix.eigenstates()\n",
    "    return matrix, (eigenpair[0][0], eigenpair[1][0])\n",
    "\n",
    "\n",
    "def pauli_kron_state_product(pauli_string: PauliTerm, vec1: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        THIS MUST BE USED FOR KRONECKER PRODUCT OF PAULI MATRICES\n",
    "        Using generalized vec-trick optimized for Pauli matrices for kron-vec multiplication\n",
    "    \"\"\"\n",
    "    n = len(pauli_string)\n",
    "    # N = 2**n\n",
    "    # halfN = N // 2\n",
    "    vec = copy.deepcopy(vec1)\n",
    "    vec = vec.reshape((2, -1), order=\"F\")  # reshape\n",
    "    for i, p in enumerate(pauli_string[::-1]):\n",
    "        # apply Pi\n",
    "        if p == \"Z\":\n",
    "            # multiply the second row by -1\n",
    "            vec[1, :] *= -1\n",
    "        elif p == \"X\" or p == \"Y\":\n",
    "            # swap the rows\n",
    "            vec[[0, 1]] = vec[[1, 0]]\n",
    "            if p == \"Y\":\n",
    "                vec *= 1.0j\n",
    "                vec[0, :] *= -1\n",
    "        if i < n - 1:\n",
    "            vec = vec.reshape((-1, 2), order=\"C\").T\n",
    "\n",
    "    return np.ravel(vec)\n",
    "\n",
    "\n",
    "def zeta(graph: nx.MultiDiGraph, B: PauliTerm) -> d.Decimal:\n",
    "    prob = dd.compute_covered_prob(graph, 0, B)\n",
    "    return prob\n",
    "\n",
    "\n",
    "def merge_terms(P: PauliTerm, Q: PauliTerm) -> PauliTerm:\n",
    "    \"\"\"\n",
    "        Combine terms such that Lift(combine(P, Q)) = Lift(P)‚à©Lift(Q)\n",
    "    \"\"\"\n",
    "    assert len(P) == len(Q)\n",
    "    res = []\n",
    "    for p, q in zip(P, Q):\n",
    "        if p == q:\n",
    "            res.append(p)\n",
    "        elif p == 'I':\n",
    "            res.append(q)\n",
    "        elif q == 'I':\n",
    "            res.append(p)\n",
    "        else:\n",
    "            raise RuntimeError('terms not compatible')\n",
    "    return ''.join(res)\n",
    "\n",
    "def multiply_terms(P: PauliTerm, Q: PauliTerm) -> PauliTerm:\n",
    "    \"\"\"X X = Y Y = Z Z = I and X I = X I = X and Y I = I Y = Y and Z I = I Z = Z. \"\"\"\n",
    "    assert len(P) == len(Q)\n",
    "    res = []\n",
    "    for p, q in zip(P, Q):\n",
    "        if p == q:\n",
    "            res.append('I')\n",
    "        elif p == 'I':\n",
    "            res.append(q)\n",
    "        elif q == 'I':\n",
    "            res.append(p)\n",
    "        else:\n",
    "            raise RuntimeError('terms not compatible')\n",
    "    return ''.join(res)\n",
    "\n",
    "\n",
    "def g(graph: nx.MultiDiGraph, P: PauliTerm, Q: PauliTerm) -> d.Decimal:\n",
    "    zeta_P = zeta(graph, P)\n",
    "    zeta_Q = zeta(graph, Q)\n",
    "    zeta_PcapQ = zeta(graph, combine(P, Q))\n",
    "       \n",
    "    assert zeta_P != 0 and zeta_Q != 0, f'{P} {zeta_P} | {Q} {zeta_Q} | {combine(P,Q)} {zeta_PcapQ}'\n",
    "    \n",
    "    return (d.Decimal(1) / zeta_P) * (d.Decimal(1) / zeta_Q) * zeta_PcapQ\n",
    "\n",
    "    \n",
    "def one_shot_variance_coeff(H: Hamiltonian, Hmatrix: np.ndarray, graph: nx.MultiDiGraph, input_state: np.ndarray, exact_value: d.Decimal=None) -> d.Decimal:\n",
    "    \"\"\"beta is according to the coefficients\"\"\"\n",
    "    paulis = {\"I\": qutip.identity(2),\n",
    "          \"X\": qutip.sigmax(),\n",
    "          \"Y\": qutip.sigmay(),\n",
    "          \"Z\": qutip.sigmaz()}\n",
    "    \n",
    "    # the input_state is a pure state, i.e. a vector\n",
    "    rho = input_state.full() @ input_state.conj().trans().full()\n",
    "    \n",
    "    first_part = d.Decimal(0)\n",
    "    zeta_cache = {}\n",
    "    psi_bra = input_state.conj().trans().full()\n",
    "    psi_ket = input_state.full()\n",
    "    \n",
    "    # if P or Q is not in H (having an implicit coefficent of 0) we don't need to consider them\n",
    "    for P, Q in itertools.product(H, repeat=2):\n",
    "        # if P and Q are not compatible we can skip them as well\n",
    "        try:\n",
    "            merge_terms(P, Q)\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "            \n",
    "        #print(f'Checking {P} and {Q}')\n",
    "        coeffs = H[P] * H[Q] # ùõº_ùëÉ * ùõº_ùëÑ\n",
    "        \n",
    "        if coeffs == 0:\n",
    "            continue\n",
    "        \n",
    "        #g_PQ = g(graph, P, Q)\n",
    "        \n",
    "        if P not in zeta_cache:\n",
    "            zeta_cache[P] = zeta(graph, P)\n",
    "        zeta_P = zeta_cache[P]\n",
    "        if Q not in zeta_cache:\n",
    "            zeta_cache[Q] = zeta(graph, Q)\n",
    "        zeta_Q = zeta_cache[Q]\n",
    "        \n",
    "        if merge_terms(P, Q) not in zeta_cache:\n",
    "            zeta_cache[merge_terms(P, Q)] = zeta(graph, merge_terms(P, Q))\n",
    "        zeta_PcapQ = zeta_cache[merge_terms(P, Q)]\n",
    "        \n",
    "        #zeta_P = zeta(graph, P)\n",
    "        #zeta_Q = zeta(graph, Q)\n",
    "        #zeta_PcapQ = zeta(graph, merge_terms(P, Q))\n",
    "                \n",
    "        if zeta_P == 0 or zeta_Q == 0 or zeta_PcapQ == 0:\n",
    "            continue\n",
    "\n",
    "        #g_PQ = (d.Decimal(1) / zeta_P) * (d.Decimal(1) / zeta_Q) * zeta_PcapQ\n",
    "        g_PQ = zeta_PcapQ / min(zeta_P, zeta_Q) / max(zeta_P, zeta_Q)\n",
    "        \n",
    "        #P_matrix = qutip.tensor( [ paulis[p] for p in P ] ).full()\n",
    "        #Q_matrix = qutip.tensor( [ paulis[p] for p in Q ] ).full()\n",
    "        #trace = np.trace(P_matrix @ Q_matrix @ rho)\n",
    "        \n",
    "        \n",
    "        p_alpha_psi_ket = np.atleast_2d(pauli_kron_state_product(multiply_terms(P, Q), psi_ket)).T\n",
    "        trace = (psi_bra @ p_alpha_psi_ket)[0][0]\n",
    "        \n",
    "        #print(trace)\n",
    "        \n",
    "        first_part += coeffs * g_PQ * d.Decimal(trace.real)\n",
    "\n",
    "    if exact_value:\n",
    "        second_part = exact_value\n",
    "    else:\n",
    "        second_part = np.trace(np.matmul(Hmatrix.full(), rho))\n",
    "        assert second_part.imag <= 0.001, f'second part == {second_part}'\n",
    "        second_part = d.Decimal(second_part.real)\n",
    "\n",
    "    return first_part, second_part, first_part - second_part**2\n",
    "\n",
    "\n",
    "def one_shot_variance_paths(H: Hamiltonian, Hmatrix: np.ndarray, graph: nx.MultiDiGraph, input_state: np.ndarray, exact_value: d.Decimal=None) -> d.Decimal:\n",
    "    \"\"\"beta is according to the coefficients\"\"\"\n",
    "    paulis = {\"I\": qutip.identity(2),\n",
    "          \"X\": qutip.sigmax(),\n",
    "          \"Y\": qutip.sigmay(),\n",
    "          \"Z\": qutip.sigmaz()}\n",
    "    \n",
    "    # the input_state is a pure state, i.e. a vector\n",
    "    rho = input_state.full() @ input_state.conj().trans().full()\n",
    "    total_paths = len(dd.dfs(graph, 0, '', set()))\n",
    "    \n",
    "    first_part = d.Decimal(0)\n",
    "    zeta_cache = {}\n",
    "    psi_bra = input_state.conj().trans().full()\n",
    "    psi_ket = input_state.full()\n",
    "    \n",
    "    # if P or Q is not in H (having an implicit coefficent of 0) we don't need to consider them\n",
    "    for P, Q in itertools.product(H, repeat=2):\n",
    "        # if P and Q are not compatible we can skip them as well\n",
    "        try:\n",
    "            merge_terms(P, Q)\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "            \n",
    "        #print(f'Checking {P} and {Q}')\n",
    "        coeffs = H[P] * H[Q] # ùõº_ùëÉ * ùõº_ùëÑ\n",
    "        \n",
    "        if coeffs == 0:\n",
    "            continue\n",
    "        \n",
    "        #g_PQ = g(graph, P, Q)\n",
    "        \n",
    "        if P not in zeta_cache:\n",
    "            zeta_cache[P] = d.Decimal(dd.count_covering_paths(graph, 0, P)) / total_paths\n",
    "        zeta_P = zeta_cache[P]\n",
    "        if Q not in zeta_cache:\n",
    "            zeta_cache[Q] = d.Decimal(dd.count_covering_paths(graph, 0, Q)) / total_paths\n",
    "        zeta_Q = zeta_cache[Q]\n",
    "        if merge_terms(P, Q) not in zeta_cache:\n",
    "            zeta_cache[merge_terms(P, Q)] = d.Decimal(dd.count_covering_paths(graph, 0, merge_terms(P, Q))) / total_paths\n",
    "        zeta_PcapQ = zeta_cache[merge_terms(P, Q)]\n",
    "        \n",
    "        # It can happen, that a PauliTerm from Hdict is not in the DD\n",
    "        # A few lines below this would lead to a division by 0 error\n",
    "        if zeta_P == 0 or zeta_Q == 0:\n",
    "            continue\n",
    "\n",
    "        g_PQ = (d.Decimal(1) / zeta_P) * (d.Decimal(1) / zeta_Q) * zeta_PcapQ\n",
    "        \n",
    "        #P_matrix = qutip.tensor( [ paulis[p] for p in P ] ).full()\n",
    "        #Q_matrix = qutip.tensor( [ paulis[p] for p in Q ] ).full()\n",
    "        #trace = np.trace(P_matrix @ Q_matrix @ rho)\n",
    "        \n",
    "        p_alpha_psi_ket = np.atleast_2d(pauli_kron_state_product(multiply_terms(P,Q), psi_ket)).T\n",
    "        trace = (psi_bra @ p_alpha_psi_ket)[0][0]\n",
    "        \n",
    "        #print(trace)\n",
    "        \n",
    "        first_part += coeffs * g_PQ * d.Decimal(trace.real)\n",
    "    \n",
    "    if exact_value:\n",
    "        second_part = exact_value\n",
    "    else:\n",
    "        second_part = np.trace(np.matmul(Hmatrix.full(), rho))\n",
    "        assert second_part.imag <= 0.001, f'second part == {second_part}'\n",
    "        second_part = d.Decimal(second_part.real)\n",
    "\n",
    "    return first_part, second_part, first_part - second_part**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm to compute optimal $\\mathbf{p}$ minimizing variances\n",
    "\n",
    "Input: Hambiltonian $H$, Decision Diagram $g$, small $\\delta > 0$ (e.g. 0.005). Basically, $\\mathbf{p}$ is the edge weights in the graph.\n",
    "\n",
    "- $v$ is a vertex and $v_i$ with $i \\in \\{X, Y, Z\\}$ is an out-edge from vertex $v$\n",
    "- $\\Pr_g[v_i]$ is the edge weight of edge $v_i$ in graph $g$. Given $v$: $\\sum_{i \\in \\operatorname{out}(v)}\\Pr_g[v_i] = 1$  \n",
    "  $\\Pr_g(b)$ is the probability of path $b \\in \\{X, Y, Z\\}^n$ in graph g  \n",
    "  graph $g'$ is a copy of $g$ where the weights are adjusted (think $\\mathbf{p}'$)\n",
    "- $\\zeta_g(Q) = \\sum_{b \\in \\operatorname{Lift}(Q)}{\\Pr}_g(b)$\n",
    "- $\\zeta_g(Q, v_i) = \\sum_{b \\in \\operatorname{Lift}(Q) \\text{ and path contains } v_i}{\\Pr}_g(b)$\n",
    "- $T_{g}(H, v_i) = \\sum_{Q \\in H} (\\alpha_Q)^2 \\frac{\\zeta_g(Q, v_i)}{(\\zeta_g(Q))^2}$\n",
    "\n",
    "Initialize edge weights in $g$, using heuristic or just random. \n",
    "\n",
    "Repeat the following steps until $g$ converges  \n",
    "- for each vertex $v$ in $g$\n",
    "  - calculate normalizer:  \n",
    "    $$\\lambda_v = \\sum_{v_i \\in \\operatorname{out}(v)} \\sum_{Q\\in H} (\\alpha_Q)^2 \\frac{\\zeta_g(Q, v_i)}{(\\zeta_g(Q))^2} \n",
    "        = \\sum_{v_i \\in \\operatorname{out}(v)} T_g(H, v_i)$$\n",
    "\n",
    "  - for each edge $v_i$ in $g$ with $i \\in \\{X,Y,Z\\}$:  \n",
    "    $${\\Pr}_{g'}[v_i] = \\frac{1}{\\lambda_v} \\sum_{Q\\in H} (\\alpha_Q)^2 \\frac{\\zeta(Q, v_i)}{(\\zeta(Q))^2}\n",
    "        = \\frac{T_{g}(H, v_i)}{T_{g}(H, v_X) + T_{g}(H, v_Y) + T_{g}(H, v_Z)}$$ \n",
    "\n",
    "- update $g$ for all edges $v_i$: ${\\Pr}_g[v_i] \\gets (1-\\delta)\\cdot {\\Pr}_g[v_i] + \\delta\\cdot{\\Pr}_{g'}[v_i]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def T(graph: nx.MultiDiGraph, H: Dict[PauliTerm, d.Decimal], node: Node, pauli_op: PauliOp) -> d.Decimal:\n",
    "    res_sum = d.Decimal(0)\n",
    "    for Q, coeff in H.items():\n",
    "        if coeff == 0:\n",
    "            continue\n",
    "        pr = dd.compute_covered_prob(graph, 0, Q)\n",
    "        pr_vi = dd.compute_covered_prob_through_edge(graph, Q, node, pauli_op)\n",
    "        #if pr > 0:\n",
    "        #    print('    {}:{:+.4f} pr={:.4f} pr({}, {})={:.4f}'.format(\n",
    "        #        Q, \n",
    "        #        coeff,\n",
    "        #        pr,\n",
    "        #        node,\n",
    "        #        pauli_op,\n",
    "        #        pr_vi\n",
    "        #    ))\n",
    "        summand = (coeff**2) * pr_vi / (pr**2)\n",
    "        #print(f'{Q} ==> ({coeff}**2) * {pr_vi} / ({pr}**2) = {summand}')\n",
    "        res_sum += summand\n",
    "    #print(f'T(..., {node}, {pauli_op}) = {res_sum}')\n",
    "    return res_sum\n",
    "\n",
    "\n",
    "def Txyz(graph: nx.MultiDiGraph, H: Dict[PauliTerm, d.Decimal], node: Node, pauli_ops: Set[PauliOp]) -> Dict[PauliOp, d.Decimal]:\n",
    "    xyz = {p: d.Decimal(0) for p in pauli_ops}\n",
    "    for Q, coeff in H.items():\n",
    "        if coeff == 0:\n",
    "            continue\n",
    "        pr = dd.compute_covered_prob(graph, 0, Q)\n",
    "        part = (coeff**2) / (pr**2)\n",
    "        \n",
    "        for i in xyz.keys():\n",
    "            pr_vi = dd.compute_covered_prob_through_edge(graph, Q, node, i)\n",
    "            xyz[i] += part * pr_vi        \n",
    "    return xyz\n",
    "\n",
    "\n",
    "def optimize_probability_distribution(graph: nx.MultiDiGraph, H: Dict[PauliTerm, d.Decimal], delta: d.Decimal, max_iterations: Optional[int]=None) -> int:\n",
    "    # new graph with same structure to serve as p'\n",
    "    new_graph = copy.deepcopy(graph)\n",
    "    H = {pt: coeff for pt, coeff in H.items() if any(p != 'I' for p in pt)}\n",
    "    \n",
    "    converged = False\n",
    "    i = 0\n",
    "    while not converged and (max_iterations is None or i < max_iterations):\n",
    "        if max_iterations is not None:\n",
    "            print(',', end='')\n",
    "        i += 1\n",
    "\n",
    "        converged = True\n",
    "        # iterate through new graph an set the weights\n",
    "        for node in new_graph.nodes:\n",
    "            #print(f'node {node}')\n",
    "            out_edges = new_graph.out_edges(node, data=True)\n",
    "            #T_cache = {data['pauli']: T(graph, H, u, data['pauli']) for u, v, data in out_edges}\n",
    "            T_cache = Txyz(graph, H, node, {data['pauli'] for _, _, data in out_edges})\n",
    "            T_normalizer = sum(T_cache.values())\n",
    "            for u, v, data in out_edges:\n",
    "                data['weight'] = T_cache[data['pauli']] / T_normalizer\n",
    "            \n",
    "            weight_out = sum(data['weight'] for u, v, data in new_graph.out_edges(node, data=True))\n",
    "            assert node == -1 or abs(weight_out - 1) < 0.00001, f'sum == {weight_out} for node {node}'\n",
    "\n",
    "        # iterate through edges of old graph and adjust the weights \n",
    "        for u, v, data in graph.edges(data=True):\n",
    "            old_weight = data['weight']\n",
    "            data['weight'] = (1-delta)*data['weight'] + delta*new_graph.edges[u,v,data['pauli']]['weight']            \n",
    "            if abs(old_weight - data['weight']) >= delta**5:\n",
    "                converged = False        \n",
    "    return i\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "import tabulate\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "import qutip\n",
    "import matplotlib.pyplot as plt\n",
    "import src.sampling as dd\n",
    "\n",
    "\n",
    "def read_hamiltonian(filename):\n",
    "    H = {}\n",
    "    lineno = 0\n",
    "    with open(filename, \"r\") as fp:\n",
    "        for line in fp:\n",
    "            if lineno % 2 == 0:\n",
    "                key = line.rstrip()\n",
    "            else:\n",
    "                val = d.Decimal(line.strip())\n",
    "                #if key in H: print(f'{key}={val} with previous {key}={H[key]}')\n",
    "                H[key] = val\n",
    "            lineno += 1\n",
    "    return H\n",
    "\n",
    "\n",
    "def convert_hamiltonian_dict_to_matrix(H, nQubits = None, eigenj = None):\n",
    "    \"\"\"converting dictionary to matrix, and returning the j-th eigenpairs\"\"\"\n",
    "    I, X, Y, Z = qutip.identity(2), qutip.sigmax(), qutip.sigmay(), qutip.sigmaz()\n",
    "    paulis = (I, X, Y, Z)\n",
    "    dPaulis = {\"I\": 0, \"X\": 1, \"Y\": 2, \"Z\": 3}\n",
    "    if nQubits is None:\n",
    "        for k in H.keys():\n",
    "            nQubits = len(k)\n",
    "            break\n",
    "    answer = None\n",
    "    for k,v in H.items():\n",
    "        if answer is None:\n",
    "            answer = float(v) * qutip.tensor( [ paulis[dPaulis[j]] for j in k ] )\n",
    "        else:\n",
    "            answer += float(v) * qutip.tensor( [ paulis[dPaulis[j]] for j in k ] )\n",
    "    eigenpair = answer.eigenstates()\n",
    "    if eigenj is None:\n",
    "        return answer, (eigenpair[0][0], eigenpair[1][0])\n",
    "    else:\n",
    "        return answer, (eigenpair[0][eigenj], eigenpair[1][eigenj])\n",
    "       \n",
    "        \n",
    "def benchmark_files(files: List[str], prune_below = 0) -> None:\n",
    "    Path('output').mkdir(exist_ok=True)\n",
    "    Path('pickle').mkdir(exist_ok=True)\n",
    "    print(f'###\\n### {sum(1 for f in filenames if f)} benchmarks \\n###\\n')\n",
    "    summary = {}\n",
    "    for filename in filenames:\n",
    "        if filename == '':\n",
    "            if summary:\n",
    "                print(tabulate.tabulate([(key, *values) for key, values in summary.items()], \n",
    "                    ['Benchmark', '|H|', '|V|', '|E|', '|E·µ•|', '|Paths|', 'Iter.', 't', 'Var 1S Paths', 'Var 1S Coeff'],\n",
    "                    tablefmt=\"simple\", floatfmt=('', '', '', '', '', '', '', '.1f', '.4f', '.4f'))\n",
    "                )\n",
    "                print('\\n')\n",
    "            continue\n",
    "        \n",
    "        start = time.time()\n",
    "        print(f'{time.strftime(\"%Y-%m-%d %H:%M:%S\")} {filename}')\n",
    "        print(f'{time.time()-start:.1f}s Reading file into dict...', end='')\n",
    "        Hdict = read_hamiltonian(filename)\n",
    "        \n",
    "        sum_alpha = sum(abs(v) for k, v in Hdict.items())\n",
    "        sum_alpha_wo_I = sum(abs(v) for k, v in Hdict.items() if any(p != 'I' for p in k))\n",
    "        print(f' |Hdict|={len(Hdict)}; Œ£|Œ±|={sum_alpha:.4f}; Œ£|Œ±|-|I|={sum_alpha_wo_I:.4f}')        \n",
    "        #for k, v in sorted(Hdict.items(), key=lambda e: abs(e[1]), reverse=True):\n",
    "        #    print(f'{k} => {v}')\n",
    "        \n",
    "        print(f'{time.time()-start:.1f}s Creating graph...', end='')\n",
    "        #graph, ppaths = dd.read_file_to_graph_with_ldf_info(Path(filename))\n",
    "        graph, ppaths = dd.read_dict_to_graph(Hdict)\n",
    "        graph.name = pathlib.Path(filename).stem\n",
    "        dd.normalize_graph(graph, ppaths)\n",
    "        \n",
    "        iterations = 0\n",
    "        # comment the next line to run without optimizations\n",
    "        iterations = optimize_probability_distribution(graph, Hdict, d.Decimal('0.05'), max_iterations=10)\n",
    "                \n",
    "        dd.write_to_dot(graph, 'output/{}.dot'.format(graph.name))\n",
    "        print(' iterations={}; |V|={}; |E|={}; |E·µ•|={}; |Paths|={}'.format(\n",
    "            iterations,\n",
    "            graph.number_of_nodes(),\n",
    "            graph.number_of_edges(),\n",
    "            sum(1 for u, v, data in graph.edges(data=True) if data.get('virtual', False)),\n",
    "            sum(1 for _ in nx.all_simple_paths(graph, 0, -1)),\n",
    "        ))\n",
    "        \n",
    "        assert dd.check_paths_and_graph_with_wildcard(graph, set(p for p in Hdict.keys() if abs(Hdict[p]) > 0))\n",
    "        \n",
    "        #for k, v in sorted(dd.graph_to_dict(graph).items(), key=lambda e: e[1], reverse=True):\n",
    "        #    print(f'{k} => {v}')\n",
    "        \n",
    "        summary[graph.name] = [\n",
    "            len(Hdict),\n",
    "            graph.number_of_nodes(),\n",
    "            graph.number_of_edges(),\n",
    "            sum(1 for u, v, data in graph.edges(data=True) if data.get('virtual', False)),\n",
    "            sum(1 for _ in nx.all_simple_paths(graph, 0, -1)),\n",
    "            iterations,\n",
    "        ]\n",
    "         \n",
    "        #print(f'{time.time()-start:.2f}s done\\n')\n",
    "        #continue\n",
    "        \n",
    "        matrix_path = Path(f'pickle/{graph.name}.pickle')\n",
    "        if matrix_path.exists():\n",
    "            print(f'{time.time()-start:.1f}s Restoring matrix/eigenstate from {matrix_path}...', end='')\n",
    "            Hmatrix, eigenpair = pickle.load( matrix_path.open('rb') )\n",
    "            print(f' exact value={eigenpair[0]}')\n",
    "        else:\n",
    "            print(f'{time.time()-start:.1f}s Converting hamiltonian dict...', end='')\n",
    "            Hmatrix, eigenpair = convert_hamiltonian_dict_to_matrix(Hdict)\n",
    "            print(f' exact value={eigenpair[0]}; |Hdict|={len(Hdict)}')\n",
    "            \n",
    "            print(f'{time.time()-start:.1f}s Dumping matrix/eigenpair to {matrix_path}...', end='')\n",
    "            pickle.dump( (Hmatrix, eigenpair), matrix_path.open('wb'))\n",
    "            print(f' done')\n",
    "        \n",
    "        print(f'{time.time()-start:.1f}s Running one-path-uniform variance...', end='')\n",
    "        var_oneshot_paths = one_shot_variance_paths(Hdict, Hmatrix, graph, eigenpair[1], exact_value=d.Decimal(eigenpair[0]))\n",
    "        print(f' p1={var_oneshot_paths[0]:.7f}; p2={var_oneshot_paths[1]:.7f}; var={var_oneshot_paths[2]:.7f}')\n",
    "                \n",
    "        print(f'{time.time()-start:.1f}s Running one-coefficients variance...', end='')\n",
    "        var_oneshot_coeff = one_shot_variance_coeff(Hdict, Hmatrix, graph, eigenpair[1], exact_value=d.Decimal(eigenpair[0]))\n",
    "        print(f' p1={var_oneshot_coeff[0]:.7f}; p2={var_oneshot_coeff[1]:.7f}; var={var_oneshot_coeff[2]:.7f}')\n",
    "        \n",
    "        print(f'{time.time()-start:.1f}s done\\n')\n",
    "        \n",
    "        summary[graph.name].extend([\n",
    "            time.time()-start,\n",
    "            var_oneshot_paths[2], \n",
    "            var_oneshot_coeff[2],\n",
    "        ])\n",
    "    \n",
    "    if summary:\n",
    "        print(tabulate.tabulate([(key, *values) for key, values in summary.items()], \n",
    "            ['Benchmark', '|H|', '|V|', '|E|', '|E·µ•|', '|Paths|', 'Iter.', 't', 'Var 1S Paths', 'Var 1S Coeff'],\n",
    "            tablefmt=\"simple\", floatfmt=('', '', '', '', '', '', '', '.1f', '.4f', '.4f'))\n",
    "        )\n",
    "    else:\n",
    "        print('Done but no summary.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "Benchmarks under `benchmarks/Hamiltonians` taken from https://github.com/rraymondhp/variances/tree/master/Hamiltonians reformated with\n",
    "```bash\n",
    "for file in */*_grouped.txt; do\n",
    "  dir=$(dirname \"$file\")\n",
    "  base=$(basename \"$file\")\n",
    "  newname=\"${dir}_${base//_grouped}\"\n",
    "  < \"$file\" awk 'BEGIN{FS=\",\"} {printf \"%s\\n%s\\n\",$2,$1;}' > \"${newname}\"\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "### 6 benchmarks \n",
      "###\n",
      "\n",
      "2021-05-10 18:42:43 hamiltonians/H2_STO3g_4qubits_jw.txt\n",
      "0.0s Reading file into dict... |Hdict|=16; Œ£|Œ±|=2.7050; Œ£|Œ±|-|I|=1.8945\n",
      "0.0s Creating graph...,,,,,,,,,, iterations=10; |V|=10; |E|=12; |E·µ•|=0; |Paths|=5\n",
      "0.3s Restoring matrix/eigenstate from pickle/H2_STO3g_4qubits_jw.pickle... exact value=-1.8572750302023784\n",
      "0.3s Running one-path-uniform variance... p1=7.6167913; p2=-1.8572750; var=4.1673207\n",
      "0.3s Running one-coefficients variance... p1=3.8475149; p2=-1.8572750; var=0.3980444\n",
      "0.3s done\n",
      "\n",
      "2021-05-10 18:42:44 hamiltonians/H2_STO3g_4qubits_parity.txt\n",
      "0.0s Reading file into dict... |Hdict|=15; Œ£|Œ±|=2.7050; Œ£|Œ±|-|I|=1.8945\n",
      "0.0s Creating graph...,,,,,,,,,, iterations=10; |V|=7; |E|=7; |E·µ•|=0; |Paths|=2\n",
      "0.2s Restoring matrix/eigenstate from pickle/H2_STO3g_4qubits_parity.pickle... exact value=-1.8572750302023784\n",
      "0.2s Running one-path-uniform variance... p1=4.0991471; p2=-1.8572750; var=0.6496766\n",
      "0.2s Running one-coefficients variance... p1=3.7497288; p2=-1.8572750; var=0.3002582\n",
      "0.2s done\n",
      "\n",
      "2021-05-10 18:42:44 hamiltonians/H2_STO3g_4qubits_bk.txt\n",
      "0.0s Reading file into dict... |Hdict|=15; Œ£|Œ±|=2.7050; Œ£|Œ±|-|I|=1.8945\n",
      "0.0s Creating graph...,,,,,,,,,, iterations=10; |V|=7; |E|=7; |E·µ•|=0; |Paths|=2\n",
      "0.2s Restoring matrix/eigenstate from pickle/H2_STO3g_4qubits_bk.pickle... exact value=-1.8572750302023784\n",
      "0.2s Running one-path-uniform variance... p1=4.0991471; p2=-1.8572750; var=0.6496766\n",
      "0.2s Running one-coefficients variance... p1=3.7497288; p2=-1.8572750; var=0.3002582\n",
      "0.2s done\n",
      "\n",
      "Benchmark                  |H|    |V|    |E|    |E·µ•|    |Paths|    Iter.    t    Var 1S Paths    Var 1S Coeff\n",
      "-----------------------  -----  -----  -----  ------  ---------  -------  ---  --------------  --------------\n",
      "H2_STO3g_4qubits_jw         16     10     12       0          5       10  0.3          4.1673          0.3980\n",
      "H2_STO3g_4qubits_parity     15      7      7       0          2       10  0.2          0.6497          0.3003\n",
      "H2_STO3g_4qubits_bk         15      7      7       0          2       10  0.2          0.6497          0.3003\n",
      "\n",
      "\n",
      "2021-05-10 18:42:44 hamiltonians/H2_6-31G_8qubits_jw.txt\n",
      "0.0s Reading file into dict... |Hdict|=224; Œ£|Œ±|=12.9772; Œ£|Œ±|-|I|=11.4519\n",
      "0.0s Creating graph...,,,,,,,,,, iterations=10; |V|=77; |E|=133; |E·µ•|=5; |Paths|=77\n",
      "284.8s Restoring matrix/eigenstate from pickle/H2_6-31G_8qubits_jw.pickle... exact value=-1.8608605555207403\n",
      "284.8s Running one-path-uniform variance... p1=17.0416945; p2=-1.8608606; var=13.5788925\n",
      "286.2s Running one-coefficients variance... p1=9.6228778; p2=-1.8608606; var=6.1600758\n",
      "287.4s done\n",
      "\n",
      "2021-05-10 18:47:31 hamiltonians/H2_6-31G_8qubits_parity.txt\n",
      "0.0s Reading file into dict... |Hdict|=214; Œ£|Œ±|=12.9772; Œ£|Œ±|-|I|=11.4519\n",
      "0.0s Creating graph...,,,,,,,,,, iterations=10; |V|=66; |E|=119; |E·µ•|=4; |Paths|=55\n",
      "213.6s Restoring matrix/eigenstate from pickle/H2_6-31G_8qubits_parity.pickle... exact value=-1.8608605555207394\n",
      "213.6s Running one-path-uniform variance... p1=40.6127153; p2=-1.8608606; var=37.1499133\n",
      "215.0s Running one-coefficients variance... p1=11.9524254; p2=-1.8608606; var=8.4896234\n",
      "216.2s done\n",
      "\n",
      "2021-05-10 18:51:08 hamiltonians/H2_6-31G_8qubits_bk.txt\n",
      "0.0s Reading file into dict... |Hdict|=211; Œ£|Œ±|=12.9772; Œ£|Œ±|-|I|=11.4519\n",
      "0.0s Creating graph...,,,,,,,,,, iterations=10; |V|=60; |E|=101; |E·µ•|=3; |Paths|=46\n",
      "109.8s Restoring matrix/eigenstate from pickle/H2_6-31G_8qubits_bk.pickle... exact value=-1.8608605555207394\n",
      "109.8s Running one-path-uniform variance... p1=29.4995031; p2=-1.8608606; var=26.0367011\n",
      "110.6s Running one-coefficients variance... p1=9.8783610; p2=-1.8608606; var=6.4155590\n",
      "111.3s done\n",
      "\n",
      "Benchmark                  |H|    |V|    |E|    |E·µ•|    |Paths|    Iter.      t    Var 1S Paths    Var 1S Coeff\n",
      "-----------------------  -----  -----  -----  ------  ---------  -------  -----  --------------  --------------\n",
      "H2_STO3g_4qubits_jw         16     10     12       0          5       10    0.3          4.1673          0.3980\n",
      "H2_STO3g_4qubits_parity     15      7      7       0          2       10    0.2          0.6497          0.3003\n",
      "H2_STO3g_4qubits_bk         15      7      7       0          2       10    0.2          0.6497          0.3003\n",
      "H2_6-31G_8qubits_jw        224     77    133       5         77       10  287.4         13.5789          6.1601\n",
      "H2_6-31G_8qubits_parity    214     66    119       4         55       10  216.2         37.1499          8.4896\n",
      "H2_6-31G_8qubits_bk        211     60    101       3         46       10  111.3         26.0367          6.4156\n"
     ]
    }
   ],
   "source": [
    "filenames = [  \n",
    "    'hamiltonians/H2_STO3g_4qubits_jw.txt', \n",
    "    'hamiltonians/H2_STO3g_4qubits_parity.txt', \n",
    "    'hamiltonians/H2_STO3g_4qubits_bk.txt',\n",
    "    \n",
    "    '', # empty string prints summary\n",
    "    \n",
    "    'hamiltonians/H2_6-31G_8qubits_jw.txt', \n",
    "    'hamiltonians/H2_6-31G_8qubits_parity.txt',\n",
    "    'hamiltonians/H2_6-31G_8qubits_bk.txt', \n",
    "    \n",
    "    #'', # empty string prints summary\n",
    "    \n",
    "    #'hamiltonians/LiH_STO3g_12qubits_jw.txt', \n",
    "    #'hamiltonians/LiH_STO3g_12qubits_parity.txt',\n",
    "    #'hamiltonians/LiH_STO3g_12qubits_bk.txt', \n",
    "    \n",
    "    #'',\n",
    "    \n",
    "    #'hamiltonians/BeH2_STO3g_14qubits_jw.txt', \n",
    "    #'hamiltonians/BeH2_STO3g_14qubits_parity.txt',\n",
    "    #'hamiltonians/BeH2_STO3g_14qubits_bk.txt', \n",
    "    \n",
    "    #'',\n",
    "    \n",
    "    #'hamiltonians/H2O_STO3g_14qubits_jw.txt', \n",
    "    #'hamiltonians/H2O_STO3g_14qubits_parity.txt',\n",
    "    #'hamiltonians/H2O_STO3g_14qubits_bk.txt', \n",
    "    \n",
    "    #'',\n",
    "    \n",
    "    #'hamiltonians/NH3_STO3g_16qubits_jw.txt', \n",
    "    #'hamiltonians/NH3_STO3g_16qubits_parity.txt', \n",
    "    #'hamiltonians/NH3_STO3g_16qubits_bk.txt', \n",
    "    \n",
    "    #'',\n",
    "    \n",
    "    #'hamiltonians/C2_STO3g_20qubits_jw.txt', \n",
    "    #'hamiltonians/C2_STO3g_20qubits_parity.txt', \n",
    "    #'hamiltonians/C2_STO3g_20qubits_bk.txt', \n",
    "    \n",
    "    #'',\n",
    "    \n",
    "    #'hamiltonians/HCl_STO3g_20qubits_jw.txt', \n",
    "    #'hamiltonians/HCl_STO3g_20qubits_parity.txt', \n",
    "    #'hamiltonians/HCl_STO3g_20qubits_bk.txt', \n",
    "]\n",
    "\n",
    "benchmark_files(filenames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
